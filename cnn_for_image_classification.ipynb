{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn_for_image_classification.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMH4CbnVnrPqPPnZWyDXl/2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mesushan/CNN-for-image-Classification/blob/master/cnn_for_image_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NcIIc_P1xro",
        "colab_type": "text"
      },
      "source": [
        "## STEP 1: SELECT RIGHT VERSION OF TENSORFLOW "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EE2zTcHEJUGW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "131f7e73-749e-4ef2-a008-6ace19c2e821"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78RhkOKM3Ld8",
        "colab_type": "text"
      },
      "source": [
        "## STEP 2: CLONE GITHUB PROJECT (FOR EASY ACCESS TO DATASET)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QCWniNOVt-5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4262e2f6-6f4b-448b-d541-c5199b7c3c48"
      },
      "source": [
        "!git clone https://github.com/mesushan/CNN-for-image-Classification.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'CNN-for-image-Classification' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Un6b6Tdxeccs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7dbcb1f9-ce3f-4774-89a7-9e1618eb748a"
      },
      "source": [
        "! ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CNN-for-image-Classification  drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9S_E5AgNJkNA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bS6MNMEzKBxs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialising the CNN\n",
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RWO-fud3jLD",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: ADDING CONVOLUTION LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1p3ZYBEKnQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 32 feature detectors with 3*3 dimensions so the convolution layer compose of 32 feature maps\n",
        "# 128 by 128 dimensions with colored image(3 channels)  (tensorflow backend)\n",
        "input_size = (128, 128)\n",
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, input_shape = (*input_size, 3), activation = 'relu'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GGnBAso3zyT",
        "colab_type": "text"
      },
      "source": [
        "## STEP 4: ADDING POOLING LAYER\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qEiPkwwULNeC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# reduce the size of feature maps and therefore reduce the number of nodes in the future fully connected layer (reduce time complexity, less compute intense without losing the performace). 2 by 2 deminsion is the recommended option\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X9HXpcCV4xFU",
        "colab_type": "text"
      },
      "source": [
        "## STEP 5: ADDING SECOND CONVOLUTION LAYER WITH POOLIING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yoh6opl-MPpA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.add(tf.keras.layers.Convolution2D(32, 3, 3, activation = 'relu'))\n",
        "model.add(tf.keras.layers.MaxPooling2D(pool_size = (2, 2)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvp8NS-85iWn",
        "colab_type": "text"
      },
      "source": [
        "## STEP 6: ADDING FLATTENING LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ryYAi7KIMd6n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# flatten all the feature maps in the pooling layer into single vector\n",
        "model.add(tf.keras.layers.Flatten())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XE8mbDLq54_p",
        "colab_type": "text"
      },
      "source": [
        "## STEP 7: ADDING A FULLY CONNECTED LAYER"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIo_G3EeMlAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# making classic ann which compose of fully connected layers\n",
        "# number of nodes in hidden layer (output_dim) (common practice is to take the power of 2)\n",
        "model.add(tf.keras.layers.Dense(units = 64, activation = 'relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.5))\n",
        "model.add(tf.keras.layers.Dense(units = 1, activation = 'sigmoid'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sbrjo20M6CXH",
        "colab_type": "text"
      },
      "source": [
        "## STEP 8: COMPILING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jwlI36vMvJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compiling the CNN\n",
        "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7CWYiHqn6SiJ",
        "colab_type": "text"
      },
      "source": [
        "## STEP 9: FITTING THE CNN TO THE IMAGES"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggxWEo0-M1h3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ed4717df-e4b5-426f-a81d-d575554d0842"
      },
      "source": [
        "# image augmentation technique to enrich our dataset(training set) without adding more images so get good performance  results with little or no overfitting even with the small amount of images\n",
        "# used from keras documentation (flow_from_directory method)\n",
        "\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "batch_size = 32\n",
        "# image augmentation part\n",
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "# create training set\n",
        "# wanna get higher accuracy -> inccrease target_size\n",
        "training_set = train_datagen.flow_from_directory('/content/CNN-for-image-Classification/dataset/training_set',\n",
        "                                                 target_size = input_size,\n",
        "                                                 batch_size = batch_size,\n",
        "                                                 class_mode = 'binary')\n",
        "\n",
        "# create test set\n",
        "# wanna get higher accuracy -> inccrease target_size\n",
        "test_set = test_datagen.flow_from_directory('/content/CNN-for-image-Classification/dataset/test_set',\n",
        "                                            target_size = input_size,\n",
        "                                            batch_size = batch_size,\n",
        "                                            class_mode = 'binary')\n",
        "\n",
        "# fit the cnn model to the trainig set and testing it on the test set\n",
        "model.fit(training_set,\n",
        "          steps_per_epoch = 8000/batch_size,\n",
        "          epochs = 35,\n",
        "          validation_data = test_set,\n",
        "          validation_steps = 2000/batch_size)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Found 8000 images belonging to 2 classes.\n",
            "Found 2000 images belonging to 2 classes.\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "WARNING:tensorflow:sample_weight modes were coerced from\n",
            "  ...\n",
            "    to  \n",
            "  ['...']\n",
            "Train for 250.0 steps, validate for 62.5 steps\n",
            "Epoch 1/35\n",
            "250/250 [==============================] - 59s 235ms/step - loss: 0.6916 - accuracy: 0.5269 - val_loss: 0.6841 - val_accuracy: 0.5655\n",
            "Epoch 2/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.6587 - accuracy: 0.6043 - val_loss: 0.6277 - val_accuracy: 0.6550\n",
            "Epoch 3/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.6286 - accuracy: 0.6499 - val_loss: 0.6402 - val_accuracy: 0.6155\n",
            "Epoch 4/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.6067 - accuracy: 0.6625 - val_loss: 0.6074 - val_accuracy: 0.6735\n",
            "Epoch 5/35\n",
            "250/250 [==============================] - 57s 230ms/step - loss: 0.5825 - accuracy: 0.6960 - val_loss: 0.5670 - val_accuracy: 0.7135\n",
            "Epoch 6/35\n",
            "250/250 [==============================] - 57s 230ms/step - loss: 0.5723 - accuracy: 0.6985 - val_loss: 0.5565 - val_accuracy: 0.7250\n",
            "Epoch 7/35\n",
            "250/250 [==============================] - 57s 229ms/step - loss: 0.5592 - accuracy: 0.7155 - val_loss: 0.5523 - val_accuracy: 0.7245\n",
            "Epoch 8/35\n",
            "250/250 [==============================] - 57s 230ms/step - loss: 0.5441 - accuracy: 0.7244 - val_loss: 0.6105 - val_accuracy: 0.6675\n",
            "Epoch 9/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5409 - accuracy: 0.7203 - val_loss: 0.5458 - val_accuracy: 0.7440\n",
            "Epoch 10/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5355 - accuracy: 0.7304 - val_loss: 0.5315 - val_accuracy: 0.7505\n",
            "Epoch 11/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5164 - accuracy: 0.7473 - val_loss: 0.5340 - val_accuracy: 0.7440\n",
            "Epoch 12/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5194 - accuracy: 0.7389 - val_loss: 0.5391 - val_accuracy: 0.7435\n",
            "Epoch 13/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5097 - accuracy: 0.7506 - val_loss: 0.5210 - val_accuracy: 0.7485\n",
            "Epoch 14/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5065 - accuracy: 0.7536 - val_loss: 0.5403 - val_accuracy: 0.7335\n",
            "Epoch 15/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.4929 - accuracy: 0.7605 - val_loss: 0.5140 - val_accuracy: 0.7560\n",
            "Epoch 16/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.5008 - accuracy: 0.7598 - val_loss: 0.5260 - val_accuracy: 0.7510\n",
            "Epoch 17/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.4943 - accuracy: 0.7605 - val_loss: 0.5090 - val_accuracy: 0.7620\n",
            "Epoch 18/35\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.4856 - accuracy: 0.7598 - val_loss: 0.5110 - val_accuracy: 0.7530\n",
            "Epoch 19/35\n",
            "250/250 [==============================] - 56s 222ms/step - loss: 0.4815 - accuracy: 0.7699 - val_loss: 0.5452 - val_accuracy: 0.7245\n",
            "Epoch 20/35\n",
            "250/250 [==============================] - 56s 223ms/step - loss: 0.4830 - accuracy: 0.7653 - val_loss: 0.4903 - val_accuracy: 0.7695\n",
            "Epoch 21/35\n",
            "250/250 [==============================] - 57s 226ms/step - loss: 0.4797 - accuracy: 0.7716 - val_loss: 0.4947 - val_accuracy: 0.7625\n",
            "Epoch 22/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.4734 - accuracy: 0.7730 - val_loss: 0.4956 - val_accuracy: 0.7690\n",
            "Epoch 23/35\n",
            "250/250 [==============================] - 57s 226ms/step - loss: 0.4745 - accuracy: 0.7724 - val_loss: 0.4967 - val_accuracy: 0.7645\n",
            "Epoch 24/35\n",
            "250/250 [==============================] - 57s 226ms/step - loss: 0.4753 - accuracy: 0.7722 - val_loss: 0.4947 - val_accuracy: 0.7575\n",
            "Epoch 25/35\n",
            "250/250 [==============================] - 56s 226ms/step - loss: 0.4728 - accuracy: 0.7734 - val_loss: 0.4946 - val_accuracy: 0.7655\n",
            "Epoch 26/35\n",
            "250/250 [==============================] - 56s 225ms/step - loss: 0.4628 - accuracy: 0.7781 - val_loss: 0.4843 - val_accuracy: 0.7710\n",
            "Epoch 27/35\n",
            "250/250 [==============================] - 56s 226ms/step - loss: 0.4583 - accuracy: 0.7847 - val_loss: 0.4919 - val_accuracy: 0.7700\n",
            "Epoch 28/35\n",
            "250/250 [==============================] - 56s 226ms/step - loss: 0.4601 - accuracy: 0.7856 - val_loss: 0.4823 - val_accuracy: 0.7765\n",
            "Epoch 29/35\n",
            "250/250 [==============================] - 57s 226ms/step - loss: 0.4506 - accuracy: 0.7886 - val_loss: 0.4909 - val_accuracy: 0.7720\n",
            "Epoch 30/35\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.4546 - accuracy: 0.7874 - val_loss: 0.4845 - val_accuracy: 0.7775\n",
            "Epoch 31/35\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.4479 - accuracy: 0.7869 - val_loss: 0.4861 - val_accuracy: 0.7735\n",
            "Epoch 32/35\n",
            "250/250 [==============================] - 58s 233ms/step - loss: 0.4528 - accuracy: 0.7894 - val_loss: 0.4733 - val_accuracy: 0.7820\n",
            "Epoch 33/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.4471 - accuracy: 0.7874 - val_loss: 0.4620 - val_accuracy: 0.7910\n",
            "Epoch 34/35\n",
            "250/250 [==============================] - 57s 228ms/step - loss: 0.4370 - accuracy: 0.7944 - val_loss: 0.4652 - val_accuracy: 0.7955\n",
            "Epoch 35/35\n",
            "250/250 [==============================] - 57s 227ms/step - loss: 0.4400 - accuracy: 0.7934 - val_loss: 0.4626 - val_accuracy: 0.7955\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcbe00f9208>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FablVLKI1ujz",
        "colab_type": "text"
      },
      "source": [
        "## STEP 10: MAKING NEW *PREDICTIONS*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zkpQ9_Bbdgug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from keras.preprocessing import image"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OARxcBiKqhaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_image = image.load_img('/content/CNN-for-image-Classification/dataset/single_prediction/cat_or_dog_4.jpg', target_size= input_size)\n",
        "test_image = image.img_to_array(test_image)\n",
        "test_image = np.expand_dims(test_image, axis = 0)\n",
        "result = model.predict(test_image)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yephYECUq3BH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aa6219cf-0719-4857-c100-becf44791bec"
      },
      "source": [
        "training_set.class_indices"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cats': 0, 'dogs': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwfR3qnDr2D2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if result [0][0] == 1:\n",
        "  prediction = 'dog'\n",
        "else:\n",
        "  prediction = 'cat'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-LuaKWdsKIn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a91c3848-85c5-4f31-b968-5ec070d6d1fa"
      },
      "source": [
        "prediction"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cat'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNTVV8xoVuMN",
        "colab_type": "text"
      },
      "source": [
        "### correct prediction made :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpVRkKU1Mo1d",
        "colab_type": "text"
      },
      "source": [
        "#### The model shows the accuracy of 79.34 percent for the training set and 79.55 for the validation set. Can be improved with further parameter tuning if needed. ex. increasing the epochs, adding more convolution layer, changing input size of image, etc."
      ]
    }
  ]
}